import os

import torch
import torch.nn as nn
from pytorch_pretrained_bert import BertModel, BertTokenizer
import gradio

# è¯†åˆ«çš„ç±»å‹
key_path = './THUCNews/data/class.txt'
key = {}

with open(key_path, mode='r') as f:
    for i, key_name in enumerate([x.strip() for x in f.readlines()]):
        key[i] = key_name

print(key)


class Config:
    """é…ç½®å‚æ•°"""

    def __init__(self):
        cru = os.path.dirname(__file__)
        self.class_list = [str(i) for i in range(len(key))]  # ç±»åˆ«åå•
        
        self.save_path = './THUCNews/saved_dict/bert.ckpt'
        self.device = torch.device('cpu')
        self.require_improvement = 1000  # è‹¥è¶…è¿‡1000batchæ•ˆæœè¿˜æ²¡æå‡ï¼Œåˆ™æå‰ç»“æŸè®­ç»ƒ
        self.num_classes = len(self.class_list)  # ç±»åˆ«æ•°
        self.num_epochs = 3  # epochæ•°
        self.batch_size = 24  # mini-batchå¤§å°
        self.pad_size = 32  # æ¯å¥è¯å¤„ç†æˆçš„é•¿åº¦(çŸ­å¡«é•¿åˆ‡)
        self.learning_rate = 5e-5  # å­¦ä¹ ç‡
        self.bert_path = './bert_pretrain'
        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)
        self.hidden_size = 768

    def build_dataset(self, text):
        lin = text.strip()
        pad_size = len(lin)
        token = self.tokenizer.tokenize(lin)
        token = ['[CLS]'] + token
        token_ids = self.tokenizer.convert_tokens_to_ids(token)
        mask=[]
        if len(token) < pad_size:
            mask = [1] * len(token_ids) + [0] * (pad_size - len(token))
            token_ids += ([0] * (pad_size - len(token)))
        else:
            mask = [1] * pad_size
            token_ids = token_ids[:pad_size]
        return torch.tensor([token_ids], dtype=torch.long), torch.tensor([mask])


class Model(nn.Module):

    def __init__(self, config):
        super(Model, self).__init__()
        self.bert = BertModel.from_pretrained(config.bert_path)
        for param in self.bert.parameters():
            param.requires_grad = True
        self.fc = nn.Linear(config.hidden_size, config.num_classes)

    def forward(self, x):
        context = x[0]
        mask = x[1]
        _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)
        out = self.fc(pooled)
        return out


config = Config()
model = Model(config).to(config.device)
model.load_state_dict(torch.load(config.save_path, map_location='cuda:2'))


def prediction_model(text):
    """è¾“å…¥ä¸€å¥é—®è¯é¢„æµ‹"""
    data = config.build_dataset(text)
    with torch.no_grad():
        outputs = model(data)
        num = torch.argmax(outputs)
    return key[int(num)]


def pred_gradio(inp):
    return prediction_model(f'{inp}')

if __name__ == '__main__':
    print(prediction_model('è¿™çŒ«å·®ç‚¹è¢«ğŸ’©æ†‹æ­»ï¼Œæ¥åŒ»é™¢çš„æ—¶å€™ä¸¥é‡é…¸ä¸­æ¯’ï¼Œé«˜ç‚ç—‡ï¼Œä½ä½“æ¸©34åº¦ï¼Œä¸å¤¸å¼ çš„è¯´ï¼Œèƒ½æ•‘æ´»çš„æ¦‚ç‡ä¸è¶…3æˆï¼Œè¿™æ¬¡å°±ä¸æ˜¯å’±åŒ»æœ¯å¥½äº†ï¼Œå…¨é å°å®¶ä¼™å‘½ç¡¬#çŒ«å’ª#ä¸€çº¿å…½åŒ»å·¥ä½œè€…'))
    inputs = gradio.inputs.Textbox(type='text')
    outputs = gradio.outputs.Textbox()
    gradio.Interface(
        fn=prediction_model, inputs=inputs, outputs=outputs
        ).launch(server_name='0.0.0.0', share=True, server_port=7675)
    